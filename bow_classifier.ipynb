{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6623241e-1229-4c67-a30f-33ab5c110e37",
   "metadata": {},
   "source": [
    "- data from [link](https://lazyprogrammer.me/course_files/deepnlp_classification_data.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cae0039-0cfe-409c-be97-79fcdb467762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d09b9e4-79f1-4332-bd2e-eb7769ed63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier :\n",
    "    def __init__(self, embedding_type = 'word2vec') :\n",
    "        self.embedding = None\n",
    "        self.type = None\n",
    "        self.stop_words = [line for line in open(os.path.join('..', 'NLP', 'data', 'stopwords.txt'), encoding='utf-8')]\n",
    "        self.stop_words = set((stopwords.words('english') + self.stop_words))\n",
    "        self.model = None\n",
    "        self.word_net_lemmatizer = WordNetLemmatizer()\n",
    "        self.type = embedding_type\n",
    "        if embedding_type == 'word2vec' :\n",
    "            self.embedding = KeyedVectors.load_word2vec_format(\n",
    "                os.path.join('data', 'GoogleNews-vectors-negative300.bin'),\n",
    "                binary=True\n",
    "            )\n",
    "            \n",
    "            \n",
    "        else :\n",
    "            self.embedding = {}\n",
    "            with open(os.path.join('data', 'glove.6B', 'glove.6B.50d.txt'), encoding='utf-8') as f:\n",
    "                for line in f :\n",
    "                    split = line.split()\n",
    "                    self.embedding[split[0]] = np.fromiter(split[1:], dtype=np.float32)\n",
    "\n",
    "    \n",
    "    def my_tokenize(self, s) :\n",
    "        #s = s.lower()\n",
    "        #s = re.sub(r\"\\d\", \"\", s)\n",
    "        #s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "        #tokens = nltk.tokenize.word_tokenize(s)\n",
    "        #tokens = [self.word_net_lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        tokens = s.split()\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, X, Y, classifier = None) :\n",
    "        '''\n",
    "        Trains on list of document strings\n",
    "        -------------------------------\n",
    "        params :\n",
    "        \n",
    "        X : List of documents(strings)\n",
    "        Y : List of correspoding outputs\n",
    "        '''\n",
    "        print('Training...')\n",
    "        self.Y2idx = {}\n",
    "        i = 0\n",
    "        Y_vectorized = []\n",
    "        for y in Y:\n",
    "            if y not in self.Y2idx :\n",
    "                self.Y2idx[y] = i\n",
    "                i += 1\n",
    "            Y_vectorized.append(self.Y2idx[y])\n",
    "        \n",
    "        fv_means = []\n",
    "        for document in X :\n",
    "            x = document.lower()\n",
    "            tokens = self.my_tokenize(x)\n",
    "            mean = 0\n",
    "            N = 0\n",
    "            for token in tokens :\n",
    "                if token in self.embedding :\n",
    "                    mean += self.embedding[token]\n",
    "                    N += 1\n",
    "            mean /= N\n",
    "            fv_means.append(mean)\n",
    "        fv_means = np.array(fv_means)\n",
    "        print(f'Post processed input shape : {fv_means.shape}')\n",
    "        \n",
    "        if classifier == 'randomforest' :\n",
    "            print('Using Random Forest Classifier')\n",
    "            self.model = RandomForestClassifier()\n",
    "        elif classifier == 'decisiontree' :\n",
    "            print('Using Decision Tree Classifier')\n",
    "            self.model = DecisionTreeClassifier()\n",
    "        elif classifier == 'logisticregression' :\n",
    "            print('Using Logistic Regression Classifier')\n",
    "            self.model = LogisticRegression(max_iter=5000)\n",
    "        else :\n",
    "            print('Using Extra Trees Classifier')\n",
    "            self.model = ExtraTreesClassifier()\n",
    "        \n",
    "        self.model.fit(fv_means, Y_vectorized)\n",
    "        \n",
    "    def score(self, X_test, Y_test) :\n",
    "        Y_vectorized = []\n",
    "        for y in Y_test:\n",
    "            if y not in self.Y2idx :\n",
    "                self.Y2idx[y] = i\n",
    "                i += 1\n",
    "            Y_vectorized.append(self.Y2idx[y])\n",
    "        \n",
    "        fv_means = []\n",
    "        for document in X_test :\n",
    "            x = document.lower()\n",
    "            tokens = self.my_tokenize(x)\n",
    "            mean = 0\n",
    "            N = 0\n",
    "            for token in tokens :\n",
    "                if token in self.embedding :\n",
    "                    mean += self.embedding[token]\n",
    "                    N += 1\n",
    "            mean /= N\n",
    "            fv_means.append(mean)\n",
    "        fv_means = np.array(fv_means)\n",
    "        return self.model.score(fv_means, Y_vectorized)\n",
    "    \n",
    "    def predict(self, X_test) :\n",
    "        fv_means = []\n",
    "        for document in X_test :\n",
    "            x = document.lower()\n",
    "            tokens = self.my_tokenize(x)\n",
    "            mean = 0\n",
    "            N = 0\n",
    "            for token in tokens :\n",
    "                if token in self.embedding :\n",
    "                    mean += self.embedding[token]\n",
    "                    N += 1\n",
    "            mean /= N\n",
    "            fv_means.append(mean)\n",
    "        fv_means = np.array(fv_means)\n",
    "        \n",
    "        predictions = self.model.predict(fv_means)\n",
    "        idx2Y = dict((v,k) for k,v in self.Y2idx.items())\n",
    "        return [idx2Y[prediction] for prediction in predictions]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06ffa5e-fa01-47d5-9c97-5c41af121688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5485 training samples\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "with open(os.path.join('data', 'deepnlp_classification_data', 'r8-train-all-terms.txt'), encoding = 'utf-8') as f :\n",
    "    for line in f :\n",
    "        y, x = line.split('\\t')\n",
    "        Y.append(y)\n",
    "        X.append(x)\n",
    "print(f'Found {len(Y)} training samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314e5171-ffab-451a-8b4c-f3611be4e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2189 testing samples\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "with open(os.path.join('data', 'deepnlp_classification_data', 'r8-test-all-terms.txt'), encoding = 'utf-8') as f :\n",
    "    for line in f :\n",
    "        y, x = line.split('\\t')\n",
    "        Y_test.append(y)\n",
    "        X_test.append(x)\n",
    "print(f'Found {len(Y_test)} testing samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17233998-ba29-4305-b54b-7bbe5989805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingClassifier(embedding_type='glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a82351e0-c8aa-499c-a2b5-5b274f1dbd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Post processed input shape : (5485, 50)\n",
      "Using Extra Trees Classifier\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, classifier='extratrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b7f08b2-7061-41dc-98ff-df1f055af2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992707383773929"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87711ce4-796f-438e-816a-cf3efba3178e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337597076290544"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a79bedc7-1ab9-4785-a1f0-7eb1816440f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : acq Predicted : acq\n",
      "Text : dominion textile calls report of bid for burlington rumor \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue again? y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : acq Predicted : acq\n",
      "Text : thomson grand public takes over thorn emi s audiovisual division thomson \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue again? n\n"
     ]
    }
   ],
   "source": [
    "while True :\n",
    "    random_x_idx = np.random.choice(len(X_test))\n",
    "    predicted_Y = model.predict([X_test[random_x_idx]])[0]\n",
    "    true_Y = Y_test[random_x_idx]\n",
    "    print(f'True : {true_Y} Predicted : {predicted_Y}')\n",
    "    print(f'Text : {X_test[random_x_idx]}')\n",
    "    continue_ = input('Continue again?')\n",
    "    if continue_.lower() != 'y' :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5e195-fc69-49bd-b341-8878ad5fdd62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
